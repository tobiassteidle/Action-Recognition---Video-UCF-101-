{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tqdm\n",
    "import heapq\n",
    "import time\n",
    "import datetime\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = 'data/UCF-101'\n",
    "VIDEOS_PATH = os.path.join(BASE_PATH, '**','*.avi')\n",
    "SEQUENCE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Extract features from videos and cache them in files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 'SEQUENCE_LENGTH' frames from each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_generator():\n",
    "    video_paths = tf.io.gfile.glob(VIDEOS_PATH)\n",
    "    np.random.shuffle(video_paths)\n",
    "    for video_path in video_paths:\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        sample_every_frame = max(1, num_frames // SEQUENCE_LENGTH)\n",
    "        current_frame = 0\n",
    "\n",
    "        label = os.path.basename(os.path.dirname(video_path))\n",
    "\n",
    "        max_images = SEQUENCE_LENGTH\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            if current_frame % sample_every_frame == 0:\n",
    "                # OPENCV reads in BGR, tensorflow expects RGB so we invert the order\n",
    "                frame = frame[:, :, ::-1]\n",
    "                img = tf.image.resize(frame, (299, 299))\n",
    "                img = tf.keras.applications.inception_v3.preprocess_input(\n",
    "                    img)\n",
    "                max_images -= 1\n",
    "                yield img, video_path\n",
    "\n",
    "            if max_images == 0:\n",
    "                break\n",
    "            current_frame += 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(frame_generator,\n",
    "             output_types=(tf.float32, tf.string),\n",
    "             output_shapes=((299, 299, 3), ()))\n",
    "\n",
    "dataset = dataset.batch(16).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Features from videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_v3 = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "x = inception_v3.output\n",
    "\n",
    "# We add Average Pooling to transform the feature map from\n",
    "# 8 * 8 * 2048 to 1 x 2048, as we don't need spatial information\n",
    "pooling_output = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "feature_extraction_model = tf.keras.Model(inception_v3.input, pooling_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features and store them in .npy filesÂ¶\n",
    "#### Extraction takes about ~1h20 minutes on an NVIDIA 1080 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = None\n",
    "all_features = []\n",
    "\n",
    "for img, batch_paths in tqdm.tqdm(dataset):\n",
    "    batch_features = feature_extraction_model(img)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1))\n",
    "    \n",
    "    for features, path in zip(batch_features.numpy(), batch_paths.numpy()):\n",
    "        if path != current_path and current_path is not None:\n",
    "            output_path = current_path.decode().replace('.avi', '.npy')\n",
    "            np.save(output_path, all_features)\n",
    "            all_features = []\n",
    "            \n",
    "        current_path = path\n",
    "        all_features.append(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Train the LSTM on video features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS = ['UnevenBars','ApplyLipstick','TableTennisShot','Fencing','Mixing','SumoWrestling','HulaHoop','PommelHorse','HorseRiding','SkyDiving','BenchPress','GolfSwing','HeadMassage','FrontCrawl','Haircut','HandstandWalking','Skiing','PlayingDaf','PlayingSitar','FrisbeeCatch','CliffDiving','BoxingSpeedBag','Kayaking','Rafting','WritingOnBoard','VolleyballSpiking','Archery','MoppingFloor','JumpRope','Lunges','BasketballDunk','Surfing','SkateBoarding','FloorGymnastics','Billiards','CuttingInKitchen','BlowingCandles','PlayingCello','JugglingBalls','Drumming','ThrowDiscus','BaseballPitch','SoccerPenalty','Hammering','BodyWeightSquats','SoccerJuggling','CricketShot','BandMarching','PlayingPiano','BreastStroke','ApplyEyeMakeup','HighJump','IceDancing','HandstandPushups','RockClimbingIndoor','HammerThrow','WallPushups','RopeClimbing','Basketball','Shotput','Nunchucks','WalkingWithDog','PlayingFlute','PlayingDhol','PullUps','CricketBowling','BabyCrawling','Diving','TaiChi','YoYo','BlowDryHair','PushUps','ShavingBeard','Knitting','HorseRace','TrampolineJumping','Typing','Bowling','CleanAndJerk','MilitaryParade','FieldHockeyPenalty','PlayingViolin','Skijet','PizzaTossing','LongJump','PlayingTabla','PlayingGuitar','BrushingTeeth','PoleVault','Punch','ParallelBars','Biking','BalanceBeam','Swing','JavelinThrow','Rowing','StillRings','SalsaSpin','TennisSwing','JumpingJack','BoxingPunchingBag'] \n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Masking(mask_value=0.),\n",
    "    tf.keras.layers.LSTM(512, dropout=0.5, recurrent_dropout=0.5),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(LABELS), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy', 'top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = os.path.join('data', 'testlist01.txt')\n",
    "train_file = os.path.join('data', 'trainlist01.txt')\n",
    "\n",
    "with open('data/testlist01.txt') as f:\n",
    "    test_list = [row.strip() for row in list(f)]\n",
    "\n",
    "with open('data/trainlist01.txt') as f:\n",
    "    train_list = [row.strip() for row in list(f)]\n",
    "    train_list = [row.split(' ')[0] for row in train_list]\n",
    "\n",
    "def make_generator(file_list):\n",
    "    def generator():\n",
    "        np.random.shuffle(file_list)\n",
    "        for path in file_list:\n",
    "            full_path = os.path.join(BASE_PATH, path).replace('.avi', '.npy')\n",
    "\n",
    "            label = os.path.basename(os.path.dirname(path))\n",
    "            features = np.load(full_path)\n",
    "\n",
    "            padded_sequence = np.zeros((SEQUENCE_LENGTH, 2048))\n",
    "            padded_sequence[0:len(features)] = np.array(features)\n",
    "\n",
    "            transformed_label = encoder.transform([label])\n",
    "            yield padded_sequence, transformed_label[0]\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_generator(make_generator(train_list),\n",
    "                 output_types=(tf.float32, tf.int16),\n",
    "                 output_shapes=((SEQUENCE_LENGTH, 2048), (len(LABELS))))\n",
    "train_dataset = train_dataset.batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_generator(make_generator(test_list),\n",
    "                 output_types=(tf.float32, tf.int16),\n",
    "                 output_shapes=((SEQUENCE_LENGTH, 2048), (len(LABELS))))\n",
    "valid_dataset = valid_dataset.batch(16).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "    597/Unknown - 334s 560ms/step - loss: 3.5795 - accuracy: 0.1670 - top_k_categorical_accuracy: 0.3782\n",
      "Epoch 00001: val_loss improved from inf to 2.46465, saving model to model.h5\n",
      "597/597 [==============================] - 471s 789ms/step - loss: 3.5795 - accuracy: 0.1670 - top_k_categorical_accuracy: 0.3782 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 2.3720 - accuracy: 0.3779 - top_k_categorical_accuracy: 0.6811\n",
      "Epoch 00002: val_loss improved from 2.46465 to 1.68674, saving model to model.h5\n",
      "597/597 [==============================] - 354s 593ms/step - loss: 2.2391 - accuracy: 0.4226 - top_k_categorical_accuracy: 0.7272 - val_loss: 1.6867 - val_accuracy: 0.5297 - val_top_k_categorical_accuracy: 0.8255\n",
      "Epoch 3/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 1.6919 - accuracy: 0.5371 - top_k_categorical_accuracy: 0.8173\n",
      "Epoch 00003: val_loss improved from 1.68674 to 1.50367, saving model to model.h5\n",
      "597/597 [==============================] - 529s 886ms/step - loss: 1.6419 - accuracy: 0.5487 - top_k_categorical_accuracy: 0.8318 - val_loss: 1.5037 - val_accuracy: 0.5728 - val_top_k_categorical_accuracy: 0.8456\n",
      "Epoch 4/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 1.3638 - accuracy: 0.6035 - top_k_categorical_accuracy: 0.8806\n",
      "Epoch 00004: val_loss improved from 1.50367 to 1.34622, saving model to model.h5\n",
      "597/597 [==============================] - 153s 256ms/step - loss: 1.3338 - accuracy: 0.6242 - top_k_categorical_accuracy: 0.8809 - val_loss: 1.3462 - val_accuracy: 0.6246 - val_top_k_categorical_accuracy: 0.8755\n",
      "Epoch 5/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 1.1569 - accuracy: 0.6668 - top_k_categorical_accuracy: 0.9110\n",
      "Epoch 00005: val_loss improved from 1.34622 to 1.25088, saving model to model.h5\n",
      "597/597 [==============================] - 53s 88ms/step - loss: 1.1285 - accuracy: 0.6781 - top_k_categorical_accuracy: 0.9119 - val_loss: 1.2509 - val_accuracy: 0.6482 - val_top_k_categorical_accuracy: 0.8908\n",
      "Epoch 6/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.9860 - accuracy: 0.7103 - top_k_categorical_accuracy: 0.9311\n",
      "Epoch 00006: val_loss did not improve from 1.25088\n",
      "597/597 [==============================] - 53s 88ms/step - loss: 0.9879 - accuracy: 0.7116 - top_k_categorical_accuracy: 0.9275 - val_loss: 1.2517 - val_accuracy: 0.6622 - val_top_k_categorical_accuracy: 0.8993\n",
      "Epoch 7/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.9092 - accuracy: 0.7323 - top_k_categorical_accuracy: 0.9351\n",
      "Epoch 00007: val_loss improved from 1.25088 to 1.23299, saving model to model.h5\n",
      "597/597 [==============================] - 52s 88ms/step - loss: 0.8997 - accuracy: 0.7390 - top_k_categorical_accuracy: 0.9410 - val_loss: 1.2330 - val_accuracy: 0.6683 - val_top_k_categorical_accuracy: 0.9014\n",
      "Epoch 8/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.8754 - accuracy: 0.7413 - top_k_categorical_accuracy: 0.9420\n",
      "Epoch 00008: val_loss improved from 1.23299 to 1.21471, saving model to model.h5\n",
      "597/597 [==============================] - 53s 88ms/step - loss: 0.8679 - accuracy: 0.7502 - top_k_categorical_accuracy: 0.9419 - val_loss: 1.2147 - val_accuracy: 0.6846 - val_top_k_categorical_accuracy: 0.8980\n",
      "Epoch 9/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.7474 - accuracy: 0.7778 - top_k_categorical_accuracy: 0.9579\n",
      "Epoch 00009: val_loss did not improve from 1.21471\n",
      "597/597 [==============================] - 52s 88ms/step - loss: 0.7678 - accuracy: 0.7719 - top_k_categorical_accuracy: 0.9539 - val_loss: 1.3754 - val_accuracy: 0.6741 - val_top_k_categorical_accuracy: 0.8826\n",
      "Epoch 10/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.7734 - accuracy: 0.7765 - top_k_categorical_accuracy: 0.9544\n",
      "Epoch 00010: val_loss did not improve from 1.21471\n",
      "597/597 [==============================] - 52s 88ms/step - loss: 0.7754 - accuracy: 0.7768 - top_k_categorical_accuracy: 0.9531 - val_loss: 1.2929 - val_accuracy: 0.6862 - val_top_k_categorical_accuracy: 0.8948\n",
      "Epoch 11/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.7177 - accuracy: 0.7940 - top_k_categorical_accuracy: 0.9560\n",
      "Epoch 00011: val_loss did not improve from 1.21471\n",
      "597/597 [==============================] - 52s 88ms/step - loss: 0.7338 - accuracy: 0.7887 - top_k_categorical_accuracy: 0.9547 - val_loss: 1.2170 - val_accuracy: 0.7150 - val_top_k_categorical_accuracy: 0.9149\n",
      "Epoch 12/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.6909 - accuracy: 0.7958 - top_k_categorical_accuracy: 0.9624\n",
      "Epoch 00012: val_loss did not improve from 1.21471\n",
      "597/597 [==============================] - 52s 88ms/step - loss: 0.6967 - accuracy: 0.7991 - top_k_categorical_accuracy: 0.9620 - val_loss: 1.2297 - val_accuracy: 0.6989 - val_top_k_categorical_accuracy: 0.9096\n",
      "Epoch 13/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.6339 - accuracy: 0.8218 - top_k_categorical_accuracy: 0.9664\n",
      "Epoch 00013: val_loss improved from 1.21471 to 1.17553, saving model to model.h5\n",
      "597/597 [==============================] - 52s 88ms/step - loss: 0.6417 - accuracy: 0.8170 - top_k_categorical_accuracy: 0.9654 - val_loss: 1.1755 - val_accuracy: 0.7074 - val_top_k_categorical_accuracy: 0.9144\n",
      "Epoch 14/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.6600 - accuracy: 0.8157 - top_k_categorical_accuracy: 0.9653\n",
      "Epoch 00014: val_loss improved from 1.17553 to 1.13664, saving model to model.h5\n",
      "597/597 [==============================] - 52s 88ms/step - loss: 0.6619 - accuracy: 0.8158 - top_k_categorical_accuracy: 0.9669 - val_loss: 1.1366 - val_accuracy: 0.7177 - val_top_k_categorical_accuracy: 0.9130\n",
      "Epoch 15/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.5787 - accuracy: 0.8369 - top_k_categorical_accuracy: 0.9735\n",
      "Epoch 00015: val_loss did not improve from 1.13664\n",
      "597/597 [==============================] - 52s 88ms/step - loss: 0.6277 - accuracy: 0.8173 - top_k_categorical_accuracy: 0.9668 - val_loss: 1.1687 - val_accuracy: 0.7164 - val_top_k_categorical_accuracy: 0.9107\n",
      "Epoch 16/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.6533 - accuracy: 0.8191 - top_k_categorical_accuracy: 0.9701\n",
      "Epoch 00016: val_loss did not improve from 1.13664\n",
      "597/597 [==============================] - 52s 88ms/step - loss: 0.6322 - accuracy: 0.8301 - top_k_categorical_accuracy: 0.9722 - val_loss: 1.3031 - val_accuracy: 0.6994 - val_top_k_categorical_accuracy: 0.9056\n",
      "Epoch 17/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.5707 - accuracy: 0.8395 - top_k_categorical_accuracy: 0.9767\n",
      "Epoch 00017: val_loss did not improve from 1.13664\n",
      "597/597 [==============================] - 52s 88ms/step - loss: 0.5934 - accuracy: 0.8339 - top_k_categorical_accuracy: 0.9709 - val_loss: 1.4121 - val_accuracy: 0.6801 - val_top_k_categorical_accuracy: 0.8916\n",
      "Epoch 18/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.5067 - accuracy: 0.8583 - top_k_categorical_accuracy: 0.9775\n",
      "Epoch 00018: val_loss did not improve from 1.13664\n",
      "597/597 [==============================] - 53s 88ms/step - loss: 0.5467 - accuracy: 0.8407 - top_k_categorical_accuracy: 0.9746 - val_loss: 1.2215 - val_accuracy: 0.7135 - val_top_k_categorical_accuracy: 0.9117\n",
      "Epoch 19/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.5996 - accuracy: 0.8369 - top_k_categorical_accuracy: 0.9740\n",
      "Epoch 00019: val_loss did not improve from 1.13664\n",
      "597/597 [==============================] - 53s 88ms/step - loss: 0.5863 - accuracy: 0.8420 - top_k_categorical_accuracy: 0.9739 - val_loss: 1.4130 - val_accuracy: 0.7002 - val_top_k_categorical_accuracy: 0.9014\n",
      "Epoch 20/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.5714 - accuracy: 0.8443 - top_k_categorical_accuracy: 0.9772\n",
      "Epoch 00020: val_loss did not improve from 1.13664\n",
      "597/597 [==============================] - 52s 88ms/step - loss: 0.5717 - accuracy: 0.8438 - top_k_categorical_accuracy: 0.9760 - val_loss: 1.3114 - val_accuracy: 0.7045 - val_top_k_categorical_accuracy: 0.8996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.5830 - accuracy: 0.8448 - top_k_categorical_accuracy: 0.9754\n",
      "Epoch 00021: val_loss did not improve from 1.13664\n",
      "597/597 [==============================] - 53s 88ms/step - loss: 0.5717 - accuracy: 0.8501 - top_k_categorical_accuracy: 0.9763 - val_loss: 1.3466 - val_accuracy: 0.6981 - val_top_k_categorical_accuracy: 0.9022\n",
      "Epoch 22/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.5802 - accuracy: 0.8379 - top_k_categorical_accuracy: 0.9738\n",
      "Epoch 00022: val_loss did not improve from 1.13664\n",
      "597/597 [==============================] - 52s 88ms/step - loss: 0.5641 - accuracy: 0.8463 - top_k_categorical_accuracy: 0.9761 - val_loss: 1.2703 - val_accuracy: 0.7140 - val_top_k_categorical_accuracy: 0.9099\n",
      "Epoch 23/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.5447 - accuracy: 0.8485 - top_k_categorical_accuracy: 0.9793\n",
      "Epoch 00023: val_loss did not improve from 1.13664\n",
      "597/597 [==============================] - 52s 88ms/step - loss: 0.5558 - accuracy: 0.8449 - top_k_categorical_accuracy: 0.9772 - val_loss: 1.3389 - val_accuracy: 0.7161 - val_top_k_categorical_accuracy: 0.9067\n",
      "Epoch 24/100\n",
      "236/237 [============================>.] - ETA: 0s - loss: 0.5268 - accuracy: 0.8594 - top_k_categorical_accuracy: 0.9791\n",
      "Epoch 00024: val_loss did not improve from 1.13664\n",
      "597/597 [==============================] - 53s 88ms/step - loss: 0.5357 - accuracy: 0.8554 - top_k_categorical_accuracy: 0.9800 - val_loss: 1.5571 - val_accuracy: 0.7005 - val_top_k_categorical_accuracy: 0.9080\n",
      "Epoch 00024: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23ee7aa6d30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir=\"logs\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, update_freq=1000, profile_batch=0)\n",
    "checkpoint = ModelCheckpoint(\"model.h5\", verbose=1, save_best_only=True)\n",
    "earlystopping = EarlyStopping(patience=10, verbose=1)\n",
    "model.fit(train_dataset, epochs=100, callbacks=[tensorboard_callback, checkpoint, earlystopping], validation_data=valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Predict on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(path):\n",
    "    features = np.load(path)\n",
    "    padded_sequence = np.zeros((SEQUENCE_LENGTH, 2048))\n",
    "    padded_sequence[0:len(features)] = np.array(features)\n",
    "    return np.array([padded_sequence])\n",
    "\n",
    "features = load_features('data/UCF-101/SumoWrestling/v_SumoWrestling_g06_c04.npy')\n",
    "prediction = model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SumoWrestling 100%\n",
      "SalsaSpin 0%\n",
      "Fencing 0%\n",
      "BandMarching 0%\n",
      "HulaHoop 0%\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "idxs = heapq.nlargest(k, range(len(prediction[0])), prediction[0].__getitem__)\n",
    "for i in idxs:\n",
    "    label = encoder.classes_[i]\n",
    "    pct = prediction[0][i]\n",
    "    print(label+ ' ' + \"{0:.0%}\".format(pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
